{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation of a Text Document in Vector Space Model & Computing Similarity between Two Documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Number of Documents : 3\n",
      "Enter Filename  1 : \n",
      "a\n",
      "Enter Filename  2 : \n",
      "b\n",
      "Enter Filename  3 : \n",
      "c\n",
      "\n",
      "The Vocabulary\n",
      " ['deep', 'learning', 'allows', 'computational', 'models', 'composed', 'multiple', 'processing', 'layers', 'learn', 'representations', 'data', 'levels', 'abstraction', 'methods', 'dramatically', 'improved', 'speech', 'recognition', 'visual', 'object', 'detection', 'many', 'domains', 'discovery', 'genomics', 'discovers', 'intricate', 'structure', 'large', 'sets', 'using', 'backpropagation', 'algorithm', 'indicate', 'machine', 'change', 'internal', 'parameters', 'used', 'compute', 'representation', 'layer', 'previous', 'convolutional', 'nets', 'brought', 'breakthroughs', 'images', 'video', 'audio', 'whereas', 'recurrent', 'shone', 'light', 'sequential', 'text', 'powerful', 'form', 'enables', 'computers', 'solve', 'perceptual', 'problems', 'image', 'increasingly', 'making', 'entry', 'biological', 'sciences', 'artificial', 'neural', 'networks', 'use', 'discover', 'patterns', 'learns', 'concept', 'subsequent', 'build', 'higher', 'level', 'abstract', 'concepts', 'learned', 'depend', 'prior', 'automatically', 'extracts', 'features', 'simple', 'example', 'network', 'tasked', 'interpreting', 'shapes', 'would', 'recognize', 'edges', 'first', 'add', 'complex', 'hard', 'fast', 'rule', 'needed', 'constitute', 'experts', 'agree', 'two', 'required', 'underlying', 'distribution', 'modeled', 'work', 'specific', 'implementation', 'called', 'stacked', 'denoising', 'autoencoders', 'explored', 'contribute', 'demonstrating', 'kind', 'coupled', 'svm', 'improves', 'classification', 'error', 'mnist', 'usual', 'approach', 'logistic', 'regression', 'added', 'stack']\n",
      "\n",
      "The Document Vector for a.txt \n",
      " [3, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "The Document Vector for b.txt \n",
      " [4, 3, 0, 0, 0, 1, 1, 2, 4, 1, 0, 3, 0, 0, 1, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "The Document Vector for c.txt \n",
      " [2, 3, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Document Frequencies\n",
      " [3, 3, 2, 1, 1, 2, 3, 2, 2, 2, 2, 3, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "\n",
      "The Cosine Similarity between  a.txt  and  b.txt  =  0.05283139357598121\n",
      "\n",
      "\n",
      "The Cosine Similarity between  a.txt  and  c.txt  =  0.017471763436749388\n",
      "\n",
      "\n",
      "The Cosine Similarity between  b.txt  and  c.txt  =  0.002739803228689164\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords,PlaintextCorpusReader\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy import spatial\n",
    "\n",
    "# Define Stopwords\n",
    "stop_words = set(stopwords.words())\n",
    "\n",
    "names = []\n",
    "# Input: Number of Documents and Document Names\n",
    "d = int(input(\"Enter Number of Documents : \"))\n",
    "for i in range(d):\n",
    "    print(\"Enter Filename \",i+1,\": \")\n",
    "    name = input()\n",
    "    names.append(name)\n",
    "vocab = []                         # Vocabulary\n",
    "for t in range(d): \n",
    "    vcb = []                       # Temporary Vocabulary for each File\n",
    "    name = names[t]+\".txt\"\n",
    "    file = open(name,'r')\n",
    "    for i in file:\n",
    "        # Preprocessing of Data\n",
    "        token = word_tokenize(i.lower())\n",
    "        token = [word.lower() for word in token if word not in stop_words]\n",
    "        token = [word.lower() for word in token if word.isalpha()]\n",
    "        # Adding words to Temporary Vocabulary\n",
    "        for item in token:\n",
    "            vcb.append(item)\n",
    "    # Adding words to Final Vocabulary\n",
    "    for i in vcb:\n",
    "        if i not in vocab  :\n",
    "            vocab.append(i)\n",
    "    del(vcb)\n",
    "    \n",
    "# Output: Vocabulary\n",
    "print(\"\\nThe Vocabulary\\n\",vocab)\n",
    "\n",
    "# Creating Document Vectors\n",
    "DV = []\n",
    "for t in range(d):\n",
    "    name = names[t]+\".txt\"\n",
    "    file = open(name,'r')\n",
    "    vec = [0]*len(vocab)             \n",
    "    for i in file:\n",
    "        # Preprocessing of Data\n",
    "        token = word_tokenize(i.lower())\n",
    "        token = [word.lower() for word in token if word not in stop_words]\n",
    "        token = [word.lower() for word in token if word.isalpha()]\n",
    "        # Calculate Term Frequencies\n",
    "        for item in token:\n",
    "            vec[vocab.index(item)]+=1\n",
    "    # Document Vectors appended together to form a Matrix\n",
    "    DV.append(vec) \n",
    "    del(vec)\n",
    "    \n",
    "# Output: Document Vectors\n",
    "for t in range(d):\n",
    "    name = names[t]+\".txt\"\n",
    "    print(\"\\nThe Document Vector for\", name,\"\\n\",DV[t])\n",
    "    \n",
    "# Calculating Document Frequency\n",
    "DF = [0]*len(vocab) \n",
    "for i in range(len(vocab)):\n",
    "    for j in range(d):\n",
    "        if DV[j][i]>0:             # If Term Frequency > 0\n",
    "            DF[i]+=1\n",
    "# Output: Document Frequency\n",
    "print(\"\\nDocument Frequencies\\n\",DF) \n",
    "\n",
    "# Calculating Inverse Document Frequency\n",
    "IDF = []\n",
    "for i in DF:\n",
    "    IDF.append(math.log((d/i),2))\n",
    "#print(\"\\nInverse Document Frequencies\\n\",IDF)\n",
    "\n",
    "# Calculating Weight of each Term in Vocabulary\n",
    "WGT = []\n",
    "for i in range(d):\n",
    "    wt = [0]*len(vocab)\n",
    "    for j in range(len(vocab)):\n",
    "        wt[j] = DV[i][j]*IDF[j]\n",
    "    WGT.append(wt)\n",
    "    del(wt)\n",
    "#print(\"\\nWeights of Vocabulary Terms\\n\",WGT)\n",
    "\n",
    "# Calculating Cosine Similarity between Documents\n",
    "for t in range(d):\n",
    "    cos = []\n",
    "    for m in range(t,d):\n",
    "        if(m!=t):\n",
    "            name = names[t]+\".txt\"\n",
    "            name1 = names[m]+\".txt\"\n",
    "            cos_sim = 1 - spatial.distance.cosine(WGT[t], WGT[m]) \n",
    "            cos.append(cos_sim)\n",
    "            # Output: Cosine Similarity\n",
    "            print(\"\\n\\nThe Cosine Similarity between \",name,\" and \",name1,\" = \",cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your Query : Deep learning discovers intricate structure in l arge data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each lay er from the representation in the previous layer.\n",
      "\n",
      "The Cosine Similarity between Query and  a.txt  =  0.5250730209536698\n",
      "\n",
      "The Cosine Similarity between Query and  b.txt  =  0.013266487822794604\n",
      "\n",
      "The Cosine Similarity between Query and  c.txt  =  0.006727244525535969\n"
     ]
    }
   ],
   "source": [
    "# Check Cosine Similarity between an Input Query and Documents\n",
    "# Input: Query\n",
    "qr=input(\"Enter your Query : \")\n",
    "query=qr.split(\" \")\n",
    "QV=[0]*len(vocab)                 # Query Vector\n",
    "for i in vocab:\n",
    "    if i in query:\n",
    "        QV[vocab.index(i)]+=1     # Calculate Term Frequency\n",
    "wt=[0]*len(vocab)                 # Weights of Vocabulary Terms\n",
    "for j in range(len(vocab)):\n",
    "    wt[j]=QV[j]*IDF[j]\n",
    "# Calculating Cosine Similarity between Documents and Query\n",
    "cos=[]\n",
    "for i in range(d):\n",
    "    name= names[i]+\".txt\"\n",
    "    cos_sim = 1 - spatial.distance.cosine(wt, WGT[i])\n",
    "    cos.append(cos_sim)\n",
    "    # Output: Cosine Similarity\n",
    "    print(\"\\nThe Cosine Similarity between Query and \",name,\" = \",cos_sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
